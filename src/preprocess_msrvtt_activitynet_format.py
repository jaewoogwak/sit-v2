#!/usr/bin/env python3
"""
MSRVTT ë°ì´í„°ë¥¼ ActivityNet í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- ë¹„ë””ì˜¤: 1.5 FPSë¡œ ìƒ˜í”Œë§í•˜ì—¬ CLIP ViT-B/32ë¡œ ì¸ì½”ë”©
- í…ìŠ¤íŠ¸: CLIP ViT-B/32 text encoderë¡œ ì¸ì½”ë”©
- ActivityNetê³¼ ë™ì¼í•œ HDF5 êµ¬ì¡° ë° caption íŒŒì¼ í˜•ì‹ ìƒì„±
"""

import os
import json
import h5py
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Tuple
import pandas as pd

import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image


def setup_clip_model(device='cuda'):
    """CLIP ëª¨ë¸ ì´ˆê¸°í™”"""
    model_name = 'openai/clip-vit-base-patch32'
    model = CLIPModel.from_pretrained(model_name)
    processor = CLIPProcessor.from_pretrained(model_name)
    
    if device == 'cuda' and torch.cuda.is_available():
        model = model.to(device)
    else:
        device = 'cpu'
        model = model.to(device)
    
    model.eval()
    return model, processor, device


def extract_video_frames_at_fps(video_path: str, target_fps: float = 1.5) -> List[np.ndarray]:
    """ë¹„ë””ì˜¤ì—ì„œ ì§€ì •ëœ FPSë¡œ í”„ë ˆì„ ì¶”ì¶œ (OpenCV ì‚¬ìš©)"""
    if not os.path.exists(video_path):
        return []
    
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return []
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if fps <= 0 or total_frames <= 0:
            cap.release()
            return []
        
        # target_fpsì— ë§ê²Œ í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°
        frame_interval = fps / target_fps
        frame_indices = []
        
        current_idx = 0
        while current_idx < total_frames:
            frame_indices.append(int(current_idx))
            current_idx += frame_interval
        
        # í”„ë ˆì„ ì¶”ì¶œ
        frames = []
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        cap.release()
        return frames
        
    except Exception as e:
        print(f"ë¹„ë””ì˜¤ ë¡œë”© ì˜¤ë¥˜ {video_path}: {e}")
        
    return []


def encode_frames_with_clip(frames: List[np.ndarray], model, processor, device) -> np.ndarray:
    """í”„ë ˆì„ë“¤ì„ CLIPìœ¼ë¡œ ì¸ì½”ë”©"""
    if len(frames) == 0:
        return np.zeros((1, 512), dtype=np.float32)
    
    # PIL Imagesë¡œ ë³€í™˜
    pil_images = []
    for frame in frames:
        # BGR to RGB ë³€í™˜ (OpenCV í˜•ì‹)
        if frame.shape[-1] == 3:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame.astype('uint8'))
        pil_images.append(pil_image)
    
    # CLIPìœ¼ë¡œ ì¸ì½”ë”© (ë°°ì¹˜ ì²˜ë¦¬)
    batch_size = 8
    all_features = []
    
    with torch.no_grad():
        for i in range(0, len(pil_images), batch_size):
            batch_images = pil_images[i:i+batch_size]
            inputs = processor(images=batch_images, return_tensors="pt", padding=True).to(device)
            image_features = model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)  # L2 ì •ê·œí™”
            all_features.append(image_features.cpu().numpy())
    
    # ëª¨ë“  features ê²°í•©
    if all_features:
        features = np.concatenate(all_features, axis=0)
        return features.astype(np.float32)
    else:
        return np.zeros((1, 512), dtype=np.float32)


def encode_text_with_clip(texts: List[str], model, processor, device) -> Dict[str, np.ndarray]:
    """í…ìŠ¤íŠ¸ë“¤ì„ CLIPìœ¼ë¡œ ì¸ì½”ë”©"""
    text_features = {}
    batch_size = 32
    
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            inputs = processor(text=batch_texts, padding=True, truncation=True, 
                             return_tensors="pt", max_length=77).to(device)
            features = model.get_text_features(**inputs)
            features = features / features.norm(dim=-1, keepdim=True)  # L2 ì •ê·œí™”
            
            # ê° í…ìŠ¤íŠ¸ë³„ë¡œ ì €ì¥
            for j, text in enumerate(batch_texts):
                text_features[text] = features[j].cpu().numpy().astype(np.float32)
    
    return text_features


def load_msrvtt_data(json_path: str) -> Tuple[Dict, List]:
    """MSRVTT JSON ë°ì´í„° ë¡œë“œ"""
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    # ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    videos = {video['video_id']: video for video in data['videos']}
    sentences = data['sentences']
    
    return videos, sentences


def create_train_test_splits(videos: Dict, sentences: List, csv_path: str) -> Tuple[List, List]:
    """Train/Test split ìƒì„± (MSRVTT_train.9k.csv ê¸°ì¤€)"""
    import pandas as pd

    # CSVì—ì„œ train video_id ë¡œë“œ
    train_csv = pd.read_csv(csv_path)
    train_video_ids = set(train_csv['video_id'].astype(str).tolist())

    train_data = []
    test_data = []
    
    # ë¹„ë””ì˜¤ë³„ ìº¡ì…˜ ëª¨ìœ¼ê¸°
    video_captions = {}
    for sent in sentences:
        video_id = sent['video_id']
        if video_id not in video_captions:
            video_captions[video_id] = []
        video_captions[video_id].append(sent['caption'])
    
    # CSV ê¸°ì¤€ìœ¼ë¡œ train/test ë‚˜ëˆ„ê¸°
    for video_id, captions in video_captions.items():
        if video_id in train_video_ids:
            for i, caption in enumerate(captions):
                train_data.append({
                    'video_id': video_id,
                    'caption_id': f"{video_id}#enc#{i}",
                    'caption': caption
                })
        else:
            # test setì€ ì²« ë²ˆì§¸ captionë§Œ ì‚¬ìš©
            if captions:
                test_data.append({
                    'video_id': video_id,
                    'caption_id': f"{video_id}#enc#0",
                    'caption': captions[0]
                })
    
    return train_data, test_data


def process_msrvtt_videos(video_dir: str, videos: Dict, model, processor, device, 
                         output_path: str) -> Dict[str, int]:
    """MSRVTT ë¹„ë””ì˜¤ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ HDF5ë¡œ ì €ì¥"""
    
    video_frame_counts = {}
    
    with h5py.File(output_path, 'w') as f:
        for video_id, video_info in tqdm(videos.items(), desc="ë¹„ë””ì˜¤ ì²˜ë¦¬"):
            video_path = os.path.join(video_dir, f"{video_id}.mp4")
            
            # í”„ë ˆì„ ì¶”ì¶œ (1.5 FPS)
            frames = extract_video_frames_at_fps(video_path, target_fps=1.5)
            
            if frames:
                # CLIPìœ¼ë¡œ ì¸ì½”ë”©
                features = encode_frames_with_clip(frames, model, processor, device)
                video_frame_counts[video_id] = len(features)
            else:
                # ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                print(f"âš ï¸  ë¹„ë””ì˜¤ ì—†ìŒ: {video_path}")
                features = np.zeros((1, 512), dtype=np.float32)
                video_frame_counts[video_id] = 1
            
            # HDF5ì— ì €ì¥ (ActivityNet í˜•ì‹: video_id -> (N, 512))
            f[video_id] = features
    
    return video_frame_counts


def process_msrvtt_texts(train_data: List, test_data: List, model, processor, device,
                        output_path: str) -> None:
    """MSRVTT í…ìŠ¤íŠ¸ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ HDF5ë¡œ ì €ì¥"""
    
    # ëª¨ë“  ê³ ìœ  ìº¡ì…˜ ìˆ˜ì§‘
    all_captions = set()
    caption_id_to_text = {}
    
    for item in train_data + test_data:
        caption_id = item['caption_id']
        caption = item['caption']
        all_captions.add(caption)
        caption_id_to_text[caption_id] = caption
    
    print(f"ê³ ìœ  ìº¡ì…˜ {len(all_captions)}ê°œ ì¸ì½”ë”© ì¤‘...")
    
    # CLIPìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
    text_features = encode_text_with_clip(list(all_captions), model, processor, device)
    
    # HDF5ì— ì €ì¥ (ActivityNet í˜•ì‹: caption_id -> (512,))
    with h5py.File(output_path, 'w') as f:
        for caption_id, caption_text in tqdm(caption_id_to_text.items(), desc="í…ìŠ¤íŠ¸ ì €ì¥"):
            if caption_text in text_features:
                f[caption_id] = text_features[caption_text]
            else:
                # ì¸ì½”ë”© ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’
                f[caption_id] = np.zeros(512, dtype=np.float32)


def create_caption_files(train_data: List, test_data: List, output_dir: str) -> None:
    """ActivityNet í˜•ì‹ì˜ caption íŒŒì¼ë“¤ ìƒì„±"""
    
    output_path = Path(output_dir)
    text_data_dir = output_path / 'TextData'
    text_data_dir.mkdir(parents=True, exist_ok=True)
    
    # Train caption íŒŒì¼
    train_file = text_data_dir / 'msrvtttrain.caption.txt'
    with open(train_file, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(f"{item['caption_id']} {item['caption']}\n")
    
    # Test caption íŒŒì¼
    test_file = text_data_dir / 'msrvtttest.caption.txt'
    with open(test_file, 'w', encoding='utf-8') as f:
        for item in test_data:
            f.write(f"{item['caption_id']} {item['caption']}\n")
    
    # Val caption íŒŒì¼ (testì™€ ë™ì¼)
    val_file = text_data_dir / 'msrvttval.caption.txt'
    with open(val_file, 'w', encoding='utf-8') as f:
        for item in test_data:
            f.write(f"{item['caption_id']} {item['caption']}\n")
    
    print(f"âœ… Caption íŒŒì¼ë“¤ ìƒì„± ì™„ë£Œ:")
    print(f"  - Train: {len(train_data)}ê°œ ({train_file})")
    print(f"  - Test: {len(test_data)}ê°œ ({test_file})")
    print(f"  - Val: {len(test_data)}ê°œ ({val_file})")


def main():
    # ì„¤ì •
    msrvtt_video_dir = '../../msr-vtt/MSRVTT_Videos'
    msrvtt_json_path = '../../msr-vtt/MSRVTT_data.json'
    output_dir = '../../msrvtt_activitynet_format'
    csv_path = '../../msr-vtt/MSRVTT_train.9k.csv'
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ MSRVTT -> ActivityNet í˜•ì‹ ì „ì²˜ë¦¬ ì‹œì‘ (Device: {device})")
    
    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    feature_data_dir = output_path / 'FeatureData'
    text_data_dir = output_path / 'TextData'
    feature_data_dir.mkdir(parents=True, exist_ok=True)
    text_data_dir.mkdir(parents=True, exist_ok=True)
    
    # CLIP ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ“¥ CLIP ëª¨ë¸ ë¡œë”©...")
    model, processor, device = setup_clip_model(device)
    
    # MSRVTT ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ MSRVTT ë°ì´í„° ë¡œë”©...")
    videos, sentences = load_msrvtt_data(msrvtt_json_path)
    print(f"ë¹„ë””ì˜¤: {len(videos)}ê°œ, ìº¡ì…˜: {len(sentences)}ê°œ")
    
    # Train/Test split ìƒì„±
    print("ğŸ”„ Train/Test split ìƒì„±...")
    train_data, test_data = create_train_test_splits(videos, sentences, csv_path)
    print(f"Train: {len(train_data)}ê°œ, Test: {len(test_data)}ê°œ")
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬
    print("ğŸ¥ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘... (1.5 FPS ìƒ˜í”Œë§)")
    video_hdf5_path = feature_data_dir / 'new_clip_vit_32_msrvtt_vid_features.hdf5'
    video_frame_counts = process_msrvtt_videos(
        msrvtt_video_dir, videos, model, processor, device, str(video_hdf5_path)
    )
    
    # í…ìŠ¤íŠ¸ ì²˜ë¦¬
    print("ğŸ’¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘...")
    text_hdf5_path = text_data_dir / 'clip_ViT_B_32_msrvtt_query_feat.hdf5'
    process_msrvtt_texts(
        train_data, test_data, model, processor, device, str(text_hdf5_path)
    )
    
    # Caption íŒŒì¼ ìƒì„±
    print("ğŸ“ Caption íŒŒì¼ ìƒì„± ì¤‘...")
    create_caption_files(train_data, test_data, output_dir)
    
    # í†µê³„ ì¶œë ¥
    total_frames = sum(video_frame_counts.values())
    avg_frames = total_frames / len(video_frame_counts) if video_frame_counts else 0
    
    print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í„°ë¦¬: {output_dir}")
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - ì´ ë¹„ë””ì˜¤: {len(videos)}ê°œ")
    print(f"  - ì´ í”„ë ˆì„: {total_frames}ê°œ (í‰ê·  {avg_frames:.1f}í”„ë ˆì„/ë¹„ë””ì˜¤)")
    print(f"  - Train ìº¡ì…˜: {len(train_data)}ê°œ")
    print(f"  - Test ìº¡ì…˜: {len(test_data)}ê°œ")
    print(f"ğŸ¯ ActivityNetê³¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ìƒì„±ë¨!")


if __name__ == '__main__':
    main()