#!/usr/bin/env python3
"""
ìµœì í™”ëœ MSRVTT ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- ì¤‘ë³µ ì œê±° ìµœì í™”
- ë°°ì¹˜ ì²˜ë¦¬
- ìºì‹œ ì‹œìŠ¤í…œ
"""

import os
import json
import h5py
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Set
from tqdm import tqdm
import argparse
from hashlib import sha256

# CLIP ê´€ë ¨ ì„í¬íŠ¸ (Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
from transformers import CLIPModel, CLIPTokenizer
from decord import VideoReader, cpu
from torchvision.transforms import InterpolationMode, Resize, CenterCrop


def load_msrvtt_annotations(msrvtt_data_path: str, csv_path: str) -> Tuple[Dict, List]:
    """MSRVTT JSONê³¼ CSV ë°ì´í„°ë¥¼ ë¡œë“œ"""
    
    # JSON ë°ì´í„° ë¡œë“œ (ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„°)
    with open(msrvtt_data_path, 'r') as f:
        msrvtt_data = json.load(f)
    
    # CSV ë°ì´í„° ë¡œë“œ (train/test ìº¡ì…˜)
    df = pd.read_csv(csv_path)
    
    # ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    video_info = {}
    for video in msrvtt_data['videos']:
        video_id = video['video_id']
        video_info[video_id] = {
            'split': video['split'],
            'category': video['category'],
            'url': video['url']
        }
    
    # ìº¡ì…˜ ë°ì´í„° êµ¬ì¡°í™”
    captions_data = []
    for _, row in df.iterrows():
        video_id = row['video_id']
        sentence = row['sentence']
        key = row['key']
        
        captions_data.append({
            'video_id': video_id,
            'caption': sentence,
            'key': key,
            'split': video_info.get(video_id, {}).get('split', 'train')
        })
    
    return video_info, captions_data


def _caption_id(caption: str) -> str:
    """ìº¡ì…˜ì—ì„œ ê³ ìœ  ID ìƒì„±"""
    return sha256(caption.encode("utf-8")).hexdigest()[:16]


def extract_video_features_batch(video_ids: List[str], video_dir: str, 
                                model, device, max_frames: int = 12) -> Dict[str, np.ndarray]:
    """ë¹„ë””ì˜¤ í”¼ì³ë¥¼ ë°°ì¹˜ë¡œ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)"""
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ ì œì™¸
    unique_video_ids = list(set(video_ids))
    video_features = {}
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •
    img_resize = Resize(224, interpolation=InterpolationMode.BICUBIC, antialias=True)
    img_crop = CenterCrop(224)
    
    print(f"ê³ ìœ  ë¹„ë””ì˜¤ {len(unique_video_ids)}ê°œ ì²˜ë¦¬ ì¤‘...")
    
    for video_id in tqdm(unique_video_ids, desc="ë¹„ë””ì˜¤ í”¼ì³ ì¶”ì¶œ"):
        video_path = os.path.join(video_dir, f"{video_id}.mp4")
        
        if not os.path.exists(video_path):
            print(f"ê²½ê³ : {video_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            video_features[video_id] = np.zeros((max_frames, 512), dtype=np.float32)
            continue
        
        try:
            # ë¹„ë””ì˜¤ ë¦¬ë”ë¡œ í”„ë ˆì„ ì¶”ì¶œ
            vr = VideoReader(video_path, ctx=cpu(0))
            total_frames = len(vr)
            
            if total_frames == 0:
                video_features[video_id] = np.zeros((max_frames, 512), dtype=np.float32)
                continue
            
            # ê· ë“±í•˜ê²Œ í”„ë ˆì„ ìƒ˜í”Œë§
            if total_frames <= max_frames:
                indices = list(range(total_frames))
            else:
                indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)
            
            # í”„ë ˆì„ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
            frames = vr.get_batch(indices).asnumpy()  # (T, H, W, C)
            
            processed_frames = []
            for frame in frames:
                # numpy arrayë¥¼ Tensorë¡œ ë³€í™˜ í›„ ì „ì²˜ë¦¬
                frame_tensor = torch.from_numpy(frame).float() / 255.0
                frame_tensor = frame_tensor.permute(2, 0, 1)  # (C, H, W)
                frame_tensor = img_resize(frame_tensor)
                frame_tensor = img_crop(frame_tensor)
                processed_frames.append(frame_tensor)
            
            if processed_frames:
                frames_batch = torch.stack(processed_frames).to(device)  # (T, C, H, W)
                with torch.no_grad():
                    features = model.get_image_features(frames_batch)
                    features = features.cpu().numpy().astype(np.float32)
            else:
                features = np.zeros((max_frames, 512), dtype=np.float32)
            
            # max_framesì— ë§ê²Œ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
            if len(features) < max_frames:
                padding = np.zeros((max_frames - len(features), 512), dtype=np.float32)
                features = np.vstack([features, padding])
            elif len(features) > max_frames:
                features = features[:max_frames]
            
            video_features[video_id] = features
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜ {video_path}: {e}")
            video_features[video_id] = np.zeros((max_frames, 512), dtype=np.float32)
    
    return video_features


def extract_text_features_batch(captions: List[str], model, tokenizer, 
                               device, batch_size: int = 32) -> Dict[str, np.ndarray]:
    """í…ìŠ¤íŠ¸ í”¼ì³ë¥¼ ë°°ì¹˜ë¡œ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)"""
    
    # ê³ ìœ  ìº¡ì…˜ë§Œ ì¶”ì¶œ
    unique_captions = list(set(captions))
    caption_to_id = {cap: _caption_id(cap) for cap in unique_captions}
    text_features = {}
    
    print(f"ê³ ìœ  ìº¡ì…˜ {len(unique_captions)}ê°œ ì²˜ë¦¬ ì¤‘...")
    
    # ë°°ì¹˜ ì²˜ë¦¬
    for i in tqdm(range(0, len(unique_captions), batch_size), desc="í…ìŠ¤íŠ¸ í”¼ì³ ì¶”ì¶œ"):
        batch_captions = unique_captions[i:i+batch_size]
        
        try:
            # ë°°ì¹˜ í† í°í™”
            tokens = tokenizer(batch_captions, return_tensors="pt", 
                             padding=True, truncation=True).to(device)
            
            with torch.no_grad():
                batch_features = model.get_text_features(**tokens)
                batch_features = batch_features.cpu().numpy().astype(np.float32)
            
            # ê° ìº¡ì…˜ë³„ë¡œ ì €ì¥
            for j, caption in enumerate(batch_captions):
                cap_id = caption_to_id[caption]
                text_features[cap_id] = batch_features[j]
                
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ í”¼ì³ë¡œ ëŒ€ì²´
            for caption in batch_captions:
                cap_id = caption_to_id[caption]
                text_features[cap_id] = np.zeros(512, dtype=np.float32)
    
    return text_features, caption_to_id


def create_gmmformer_structure_fast(output_dir: str, video_info: Dict, captions_data: List,
                                  video_dir: str, model, tokenizer, device):
    """ìµœì í™”ëœ GMMFormer í˜¸í™˜ ë°ì´í„° êµ¬ì¡° ìƒì„±"""
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±
    text_data_dir = output_path / 'TextData'
    feature_data_dir = output_path / 'FeatureData'
    text_data_dir.mkdir(exist_ok=True)
    feature_data_dir.mkdir(exist_ok=True)
    
    # ì „ì²´ ë°ì´í„°ì—ì„œ ê³ ìœ  ë¹„ë””ì˜¤ì™€ ìº¡ì…˜ ì¶”ì¶œ
    all_video_ids = [item['video_id'] for item in captions_data]
    all_captions = [item['caption'] for item in captions_data]
    
    print(f"ì´ {len(captions_data)}ê°œ ìº¡ì…˜-ë¹„ë””ì˜¤ ìŒ")
    print(f"ê³ ìœ  ë¹„ë””ì˜¤: {len(set(all_video_ids))}ê°œ")
    print(f"ê³ ìœ  ìº¡ì…˜: {len(set(all_captions))}ê°œ")
    
    # 1. ëª¨ë“  ë¹„ë””ì˜¤ í”¼ì³ í•œë²ˆì— ì¶”ì¶œ
    print("\n=== ë¹„ë””ì˜¤ í”¼ì³ ì¶”ì¶œ ===")
    video_features = extract_video_features_batch(all_video_ids, video_dir, model, device)
    
    # 2. ëª¨ë“  í…ìŠ¤íŠ¸ í”¼ì³ í•œë²ˆì— ì¶”ì¶œ  
    print("\n=== í…ìŠ¤íŠ¸ í”¼ì³ ì¶”ì¶œ ===")
    text_features, caption_to_id = extract_text_features_batch(all_captions, model, tokenizer, device)
    
    # 3. Splitë³„ë¡œ ë°ì´í„° ë¶„ë¦¬ ë° ì €ì¥
    splits = {'train': [], 'val': [], 'test': []}
    
    for item in captions_data:
        split = item['split']
        if split in splits:
            splits[split].append(item)
        else:
            # unknownì€ trainìœ¼ë¡œ í• ë‹¹
            splits['train'].append(item)
    
    # ê° splitì— ëŒ€í•´ íŒŒì¼ ìƒì„±
    for split_name, split_data in splits.items():
        if not split_data:
            continue
            
        print(f"\n=== {split_name} ë°ì´í„° ì €ì¥ ({len(split_data)}ê°œ) ===")
        
        # ìº¡ì…˜ íŒŒì¼ ìƒì„± (GMMFormer í˜•ì‹)
        caption_file = text_data_dir / f'msrvtt_{split_name}.caption.txt'
        
        with open(caption_file, 'w', encoding='utf-8') as f:
            for item in split_data:
                key = item['key']
                caption = item['caption']
                # GMMFormer í˜•ì‹: key caption
                f.write(f"{key} {caption}\n")
    
    # 4. HDF5 íŒŒì¼ì— ëª¨ë“  í”¼ì³ ì €ì¥ (split êµ¬ë¶„ ì—†ì´)
    print("\n=== HDF5 íŒŒì¼ ì €ì¥ ===")
    
    # í…ìŠ¤íŠ¸ í”¼ì³ ì €ì¥
    text_feat_file = text_data_dir / 'clip_ViT_B_32_msrvtt_query_feat.hdf5'
    
    with h5py.File(text_feat_file, 'w') as hf:
        for item in tqdm(captions_data, desc="í…ìŠ¤íŠ¸ í”¼ì³ ì €ì¥"):
            key = item['key']
            caption = item['caption']
            cap_id = _caption_id(caption)
            if cap_id in text_features:
                hf[key] = text_features[cap_id]
            else:
                hf[key] = np.zeros(512, dtype=np.float32)
    
    # ë¹„ë””ì˜¤ í”¼ì³ ì €ì¥
    video_feat_file = feature_data_dir / 'new_clip_vit_32_msrvtt_vid_features.hdf5'
    
    with h5py.File(video_feat_file, 'w') as hf:
        for video_id, features in tqdm(video_features.items(), desc="ë¹„ë””ì˜¤ í”¼ì³ ì €ì¥"):
            hf[video_id] = features
    
    # video2frames.txt ìƒì„±
    video2frames_dir = feature_data_dir / 'clip'
    video2frames_dir.mkdir(exist_ok=True)
    video2frames_file = video2frames_dir / 'video2frames.txt'
    
    with open(video2frames_file, 'w') as f:
        for video_id in video_features.keys():
            # video2frames í˜•ì‹: video_id frame_ids
            f.write(f"{video_id} {video_id}\n")
    
    print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - ë¹„ë””ì˜¤ í”¼ì³: {len(video_features)}ê°œ")
    print(f"  - í…ìŠ¤íŠ¸ í”¼ì³: {len(text_features)}ê°œ")
    print(f"  - ìº¡ì…˜ íŒŒì¼: {len(splits)}ê°œ split")


def main():
    parser = argparse.ArgumentParser(description="ìµœì í™”ëœ MSRVTT ë°ì´í„°ì…‹ ì „ì²˜ë¦¬")
    
    parser.add_argument('--msrvtt_data', type=str, 
                       default='/disk/gjw/msr-vtt/MSRVTT_data.json',
                       help='MSRVTT JSON ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--csv_path', type=str,
                       default='/disk/gjw/msr-vtt/MSRVTT_JSFUSION_train_test_10k.csv',
                       help='MSRVTT CSV ìº¡ì…˜ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--video_dir', type=str,
                       default='/disk/gjw/msr-vtt/MSRVTT_Videos',
                       help='MSRVTT ë¹„ë””ì˜¤ ë””ë ‰í„°ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str,
                       default='/disk/gjw/msrvtt',
                       help='ì¶œë ¥ ë””ë ‰í„°ë¦¬ ê²½ë¡œ')
    parser.add_argument('--max_frames', type=int, default=12,
                       help='ë¹„ë””ì˜¤ë‹¹ ìµœëŒ€ í”„ë ˆì„ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--device', type=str, default='cuda',
                       help='CLIP ëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤')
    
    args = parser.parse_args()
    
    # CLIP ëª¨ë¸ ë¡œë“œ (Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
    print("ğŸš€ CLIP ëª¨ë¸ ë¡œë“œ ì¤‘...")
    device = args.device if torch.cuda.is_available() else 'cpu'
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ MSRVTT ë°ì´í„° ë¡œë“œ ì¤‘...")
    video_info, captions_data = load_msrvtt_annotations(args.msrvtt_data, args.csv_path)
    
    print(f"ğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„:")
    print(f"  - ë¹„ë””ì˜¤ ìˆ˜: {len(video_info)}")
    print(f"  - ìº¡ì…˜ ìˆ˜: {len(captions_data)}")
    
    # GMMFormer êµ¬ì¡°ë¡œ ë³€í™˜ (ìµœì í™”ëœ ë°©ì‹)
    create_gmmformer_structure_fast(
        args.output_dir, video_info, captions_data, args.video_dir,
        model, tokenizer, device
    )


if __name__ == '__main__':
    main()